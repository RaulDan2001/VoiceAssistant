from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import AutoModelForSeq2SeqLM

# Aleg un model pre-antrenat de pe Hugging Face pentru procesarea cuvintelor si raspuns
#model_name = "EleutherAI/gpt-neo-2.7B"
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

#Aleg un model pre-antrenat de pe Hugging Face pentru traducere
translate_model_name = "Helsinki-NLP/opus-mt-ROMANCE-en"
print("Loading tokenizer...")
translate_tokenizer = AutoTokenizer.from_pretrained(translate_model_name)
print("Tokenizer loaded.")

print("Loading model...")
translate_model = AutoModelForSeq2SeqLM.from_pretrained(translate_model_name)
print("Model loaded.")

#Pentru traducere in romana
ro_translate_model_name = "Helsinki-NLP/opus-mt-en-ROMANCE"
print("Loading tokenizer...")
ro_translate_tokenizer = AutoTokenizer.from_pretrained(ro_translate_model_name)
print("Tokenizer loaded.")

print("Loading model...")
ro_translate_model = AutoModelForSeq2SeqLM.from_pretrained(ro_translate_model_name)
print("Model loaded.")

# Setez token-ul `pad` explicit (acesta va fi identic cu `eos`)
tokenizer.pad_token = tokenizer.eos_token
translate_tokenizer.pad_token = translate_tokenizer.eos_token
ro_translate_tokenizer.pad_token = ro_translate_tokenizer.eos_token

def translate_to_english(text):
    # Tokenizez textul
    input_text = f"translate ro to en: {text}"
    inputs = translate_tokenizer(input_text, return_tensors="pt", padding=True)
    # Generez traducerea
    translated = translate_model.generate(input_ids=inputs["input_ids"],
    attention_mask=inputs["attention_mask"],
    max_new_tokens=100,  # Ensure the translation output is long enough
    do_sample=False,
    num_beams=5,    # Using beam search for better translation quality
    early_stopping=True,
    pad_token_id=translate_tokenizer.pad_token_id,  # Explicit padding token id
    eos_token_id=translate_tokenizer.eos_token_id,  # Explicit EOS token id
    )
    # Decodific rezultatul
    result = translate_tokenizer.decode(translated[0], skip_special_tokens=True) 

    result = result.replace(f"translate ro to en: ", "").replace("<pad>", "").strip()
    return result

def translate_to_romanian(text):
    #Tokenizez textul
    input_text = f"translate en to ro: {text}"
    inputs = ro_translate_tokenizer(input_text, return_tensors="pt", padding=True)
    #Generez traducerea
    translated = ro_translate_model.generate(input_ids=inputs["input_ids"],
    attention_mask=inputs["attention_mask"],
    max_new_tokens=100,  # Ensure the translation output is long enough
    do_sample=False,
    num_beams=5,    # Using beam search for better translation quality
    early_stopping=True,
    pad_token_id=translate_tokenizer.pad_token_id,  # Explicit padding token id
    eos_token_id=translate_tokenizer.eos_token_id,  # Explicit EOS token id
    )

    # Decodific rezultatul
    result = translate_tokenizer.decode(translated[0], skip_special_tokens=True) 

    result = result.replace(f"translate ro to en: ", "").replace("<pad>", "").strip()
    return result

def chat_with_assistant(prompt):
    # Convertesc întrebarea în formatul necesar pentru model
    translated_inputs = translate_to_english(prompt)
    print ("Translatet_input recevied=", translated_inputs)
    inputs = tokenizer(translated_inputs, return_tensors="pt", padding=True, truncation=True)
    # Accesez input_ids
    input_ids = inputs["input_ids"]

    # Generez un răspuns folosind parametrii pentru a evita repetitivitatea
    outputs = model.generate(
        input_ids,  # Tensorul corect
        max_new_tokens=50,
        num_return_sequences=1,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        attention_mask=inputs["attention_mask"],  # Adaug attention_mask
        do_sample=True,  # Activez sampling-ul
        top_k=50,  # Selecție restrictivă pentru cele mai probabile 50 de token-uri
        top_p=0.95,  # Nucleus sampling
        temperature=0.7,  # Temperatura mai mică reduce repetitivitatea
    )
    
    # Decodific răspunsul în text
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    #rezult = translate_to_romanian(response)
    return response

# Interacțiune continuă cu utilizatorul
def assistant_interaction():
    print("Salut de la asistent! Scrie exit pentru a ieși.")
    while True:
        user_input = input("Eu: ")
        if user_input.lower() == "exit":
            print("Asistent: La revedere!")
            break
        response = chat_with_assistant(user_input)
        print("Asistent:", response)

# Rulez aplicația
def main():
    assistant_interaction()

if __name__ == "__main__":
    main()
